<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Neural Network-01 Batch Hard and Semi-Hard Triplet Loss - Augustus&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Augustus&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Augustus&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Triplet LossTriplet Loss的介紹，相信在網路上已經有許多例子了，用途跟作用這邊就不多作介紹，將triplet的公式列出來，直接進入正題："><meta property="og:type" content="blog"><meta property="og:title" content="Neural Network-01 Batch Hard and Semi-Hard Triplet Loss"><meta property="og:url" content="https://augustushsu.github.io/2020/02/04/NeuralNetwork-01/"><meta property="og:site_name" content="Augustus&#039;s Blog"><meta property="og:description" content="Triplet LossTriplet Loss的介紹，相信在網路上已經有許多例子了，用途跟作用這邊就不多作介紹，將triplet的公式列出來，直接進入正題："><meta property="og:locale" content="zh_TW"><meta property="og:image" content="https://augustushsu.github.io/uploads/Triplet-Loss.png"><meta property="article:published_time" content="2020-02-04T19:18:26.000Z"><meta property="article:modified_time" content="2021-12-03T10:46:43.432Z"><meta property="article:author" content="Augustus Hsu"><meta property="article:tag" content="triplet loss"><meta property="article:tag" content="tensorflow"><meta property="article:tag" content="numpy"><meta property="article:tag" content="batch hard"><meta property="article:tag" content="semi-hard"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/uploads/Triplet-Loss.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://augustushsu.github.io/2020/02/04/NeuralNetwork-01/"},"headline":"Neural Network-01 Batch Hard and Semi-Hard Triplet Loss","image":["https://augustushsu.github.io/uploads/Triplet-Loss.png"],"datePublished":"2020-02-04T19:18:26.000Z","dateModified":"2021-12-03T10:46:43.432Z","author":{"@type":"Person","name":"Augustus Hsu"},"publisher":{"@type":"Organization","name":"Augustus's Blog","logo":{"@type":"ImageObject","url":{"text":"Augustus"}}},"description":"Triplet LossTriplet Loss的介紹，相信在網路上已經有許多例子了，用途跟作用這邊就不多作介紹，將triplet的公式列出來，直接進入正題："}</script><link rel="canonical" href="https://augustushsu.github.io/2020/02/04/NeuralNetwork-01/"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/xt256.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-154556648-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-154556648-1');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Augustus</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="My GitHub" href="https://augustushsu.github.io/"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="文章目錄" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜尋" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-02-04T19:18:26.000Z" title="2/4/2020, 7:18:26 PM">2020-02-04</time>發表</span><span class="level-item"><time dateTime="2021-12-03T10:46:43.432Z" title="12/3/2021, 10:46:43 AM">2021-12-03</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E5%AF%A6%E4%BD%9C/">實作</a><span> / </span><a class="link-muted" href="/categories/%E5%AF%A6%E4%BD%9C/Neural-Network/">Neural Network</a></span><span class="level-item">12 分鐘讀完 (大約1870個字)</span></div></div><h1 class="title is-3 is-size-4-mobile">Neural Network-01 Batch Hard and Semi-Hard Triplet Loss</h1><div class="content"><h2 id="Triplet-Loss"><a href="#Triplet-Loss" class="headerlink" title="Triplet Loss"></a>Triplet Loss</h2><p>Triplet Loss的介紹，相信在網路上已經有許多例子了，用途跟作用這邊就不多作介紹，將triplet的公式列出來，直接進入正題：</p>
<span id="more"></span>



$$
{\mathcal{L_{triplet}}=\max{(\sum_{i=1}^N[||f^a_i - f^p_i||_2^2-||f^a_i - f^n_i||_2^2 + margin] ,\ 0)}}
$$



<p>這邊會依序測試兩種不一樣的Triplet來計算Triplet Loss，分別是：</p>
<ol>
<li>Batch Hard</li>
<li>Semihard</li>
</ol>
<img src="https://drive.google.com/uc?export=view&id=1zXXicrjoieVZLGGg3qmcLJaryw-ZKeo4" alt="semi-hard_triplet" width="50%" height="50%" style="display:block; margin:auto;">

<h3 id="Sample-Data"><a href="#Sample-Data" class="headerlink" title="Sample Data"></a>Sample Data</h3><p>首先要先Sample出兩個不同的Embedding資料，直接使用<code>numpy</code>的<code>random</code>來產生兩個隨機的Embedding。<br>這邊設定Batch Size是64、每筆資料的維度為1024、Margin設定為0.3、另外Label這邊設定是每筆都是獨立的資料，所以每一筆資料對應一個label：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">batch = <span class="number">64</span></span><br><span class="line">emb_dim = <span class="number">1024</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1234</span>)</span><br><span class="line">emb1 = np.random.rand(batch,emb_dim).astype(np.float32)</span><br><span class="line">np.random.seed(<span class="number">2345</span>)</span><br><span class="line">emb2 = np.random.rand(batch,emb_dim).astype(np.float32)</span><br><span class="line">margin = <span class="number">0.3</span></span><br><span class="line">labels = np.arange(batch)</span><br></pre></td></tr></table></figure>

<h2 id="Triplet-Loss-Batch-Hard"><a href="#Triplet-Loss-Batch-Hard" class="headerlink" title="Triplet Loss (Batch Hard)"></a>Triplet Loss (Batch Hard)</h2><p>直接將最小的AN距離減掉最大的AP距離加上margin，此為Batch Hard的Triplet Loss。</p>
<h3 id="Distance-Metric"><a href="#Distance-Metric" class="headerlink" title="Distance Metric"></a>Distance Metric</h3><p>在<a target="_blank" rel="noopener" href="https://github.com/tensorflow/addons/blob/ad132da23a8162eb97c435676dd7426e622a0074/tensorflow_addons/losses/metric_learning.py">Tensorflow Addons原始碼</a>中使用square再加起來等價於$XX^T$取對角的element，不過不確定跟目前使用的方法計算複雜度的差距：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩陣相乘再取對角的element</span></span><br><span class="line">xx = np.matmul(x, np.transpose(x))</span><br><span class="line">xx = np.diag(xx)</span><br><span class="line"><span class="comment"># 使用square再根據axix=1相加</span></span><br><span class="line">xx = tf.math.reduce_sum(tf.math.square(x), axis=[<span class="number">1</span>], keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>針對一個batch所有的資料兩兩計算，形成一個Batch*Batch的metric：</p>
<p>這裡計算每一筆的距離，回傳的是一個shape=(batch, batch)的metric：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">np_distance_metric</span>(<span class="params">embedding, squared=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">       embedding: float32, with shape [n, d], (batch_size, d)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dist: float32, with shape [m, n], (batch_size, batch_size)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># |x-y|^2 = x^2 - 2xy + y^2</span></span><br><span class="line">    xy = np.matmul(embedding, np.transpose(embedding))</span><br><span class="line">    square_norm = np.diag(xy)</span><br><span class="line">    xx = np.expand_dims(square_norm, <span class="number">0</span>)</span><br><span class="line">    yy = np.expand_dims(square_norm, <span class="number">1</span>)</span><br><span class="line">    distances = np.add(xx, yy) - <span class="number">2.0</span> * xy</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    (batch_size,1)-(batch_size,batch_size): Equivalent to each column operation</span></span><br><span class="line"><span class="string">    (batch_size,batch_size)+(1,batch_size): Equivalent to each row operation</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># Deal with numerical inaccuracies. Set small negatives to zero.</span></span><br><span class="line">    distances = np.maximum(distances, <span class="number">0.0</span>)</span><br><span class="line">    <span class="comment"># Get the mask where the zero distances are at.</span></span><br><span class="line">    error_mask = np.less_equal(distances, <span class="number">0.0</span>).astype(np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> squared:</span><br><span class="line">        <span class="comment"># Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)</span></span><br><span class="line">        <span class="comment"># we need to add a small epsilon where distances == 0.0</span></span><br><span class="line">        distances = np.sqrt(distances + error_mask * <span class="number">1e-16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Undo conditionally adding 1e-16.</span></span><br><span class="line">    distances = np.multiply(distances, np.logical_not(error_mask),)</span><br><span class="line"></span><br><span class="line">    num_data = np.shape(embedding)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># Explicitly set diagonals to zero.</span></span><br><span class="line">    mask_offdiagonals = np.ones_like(distances) - np.diag(np.ones([num_data]))</span><br><span class="line">    distances = np.multiply(distances, mask_offdiagonals)</span><br><span class="line">    <span class="keyword">return</span> distances</span><br></pre></td></tr></table></figure>

<h3 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h3><p>_masked_minimum做的簡而言之就是將<code>乘上mask後的data中根據dim取其最小值</code>，具體步驟是：</p>
<blockquote>
<p>首先將每筆資料對應的最大值選取出來，命名為：<code>axis_maximums</code>，將Distance Metric減掉這個最大值(axis_maximums)，再乘上mask選取出需要比較的數值，接下來根據dim取出對應行或列中最小值，最後再將axis_maximum加回去就可以得到需要的結果，然後需要注意這邊有keepdims，所以會是(batch, 1)的形式。</p>
</blockquote>
<p>具體的實現過程為：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">np_masked_minimum</span>(<span class="params">data, mask, dim=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Computes the axis wise minimum over chosen elements.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      data: float32, with shape [n, m], (batch_size, batch_size)</span></span><br><span class="line"><span class="string">      mask: boolean, with shape [n, m], (batch_size, batch_size)</span></span><br><span class="line"><span class="string">      dim: int, the dimension which want to compute the minimum.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      masked_minimums: float32, with shape [n, 1], (batch_size, batch_size)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    axis_maximums = np.<span class="built_in">max</span>(data, dim, keepdims=<span class="literal">True</span>)</span><br><span class="line">    masked_minimums = (np.<span class="built_in">min</span>(np.multiply(data - axis_maximums, mask), dim, keepdims=<span class="literal">True</span>) + axis_maximums)</span><br><span class="line">    <span class="keyword">return</span> masked_minimums</span><br></pre></td></tr></table></figure>

<p>_masked_maximum做的事情和上面一樣，不過得到的為最大值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">np_masked_maximum</span>(<span class="params">data, mask, dim=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Computes the axis wise maximum over chosen elements.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      data: float32, with shape [n, m], (batch_size, batch_size)</span></span><br><span class="line"><span class="string">      mask: boolean, with shape [n, m], (batch_size, batch_size)</span></span><br><span class="line"><span class="string">      dim: int, the dimension over which to compute the maximum.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      masked_maximums: N-D `Tensor`.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    axis_minimums = tf.math.reduce_min(data, dim, keepdims=<span class="literal">True</span>)</span><br><span class="line">    masked_maximums = (tf.math.reduce_max(tf.math.multiply(data - axis_minimums, mask), dim, keepdims=<span class="literal">True</span>) + axis_minimums)</span><br><span class="line">    <span class="keyword">return</span> masked_maximums</span><br></pre></td></tr></table></figure>

<p>定義出以上function之後，就可以開始進行整個Triplet Loss的計算。</p>
<h3 id="Batch-Hard"><a href="#Batch-Hard" class="headerlink" title="Batch Hard"></a>Batch Hard</h3><p>首先先算出pairwise的距離矩陣<code>pdist_matrix</code>，再利用<code>labels</code>計算出row index和column index對應的embedding是否有相同的label(<code>adjacency</code>)，相反可以得出不同的label(<code>adjacency_not</code>)。</p>
<p>如此就可以得到negative所在的mask和positive所在的mask，利用前面的mask function找出hardest的negative，同理利用前面的<code>adjacency</code>再去掉相同index的element就可以得到positive的mask，也就是得到hardest的positive。</p>
<p>有了hardest的positive跟negative就可以直接帶入triplet的公式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">np_triplet_batch_hard</span>(<span class="params">labels, embedding, margin, soft</span>):</span></span><br><span class="line">  	<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    batch all triplet loss of a batch</span></span><br><span class="line"><span class="string">    ------------------------------------</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        labels:     Label Data, shape = (batch_size,1)</span></span><br><span class="line"><span class="string">        embedding:  embedding vector, shape = (batch_size, vector_size)</span></span><br><span class="line"><span class="string">        margin:     margin, scalar</span></span><br><span class="line"><span class="string">        soft::     	use log1p or not, boolean</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        triplet_loss: scalar, for one batch</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># Reshape label tensor to [batch_size, 1].</span></span><br><span class="line">    lshape = np.shape(labels)</span><br><span class="line">    labels = np.reshape(labels, [lshape[<span class="number">0</span>], <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># Build pairwise squared distance matrix.</span></span><br><span class="line">    pdist_matrix = _distance_metric(embedding, squared=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build pairwise binary adjacency matrix.</span></span><br><span class="line">    adjacency = np.equal(labels, tf.transpose(labels)).astype(np.float32)</span><br><span class="line">    <span class="comment"># Invert so we can select negatives only.</span></span><br><span class="line">    adjacency_not = np.logical_not(adjacency).astype(np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># hard negatives: smallest D_an.</span></span><br><span class="line">    hard_negatives = _masked_minimum(pdist_matrix, adjacency_not)</span><br><span class="line"></span><br><span class="line">    batch_size = np.size(labels)</span><br><span class="line">    mask_positives = adjacency - np.diag(np.ones([batch_size]))</span><br><span class="line">    <span class="comment"># hard positives: largest D_ap.</span></span><br><span class="line">    hard_positives = _masked_maximum(pdist_matrix, mask_positives)</span><br><span class="line">    <span class="keyword">if</span> soft:</span><br><span class="line">        triplet_loss = np.log1p(np.exp(hard_positives - hard_negatives))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        triplet_loss = np.maximum(hard_positives - hard_negatives + margin, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get final mean triplet loss</span></span><br><span class="line">    triplet_loss = np.mean(triplet_loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> triplet_loss</span><br></pre></td></tr></table></figure>



<h2 id="Triplet-Loss-Semi-Hard"><a href="#Triplet-Loss-Semi-Hard" class="headerlink" title="Triplet Loss (Semi-Hard)"></a>Triplet Loss (Semi-Hard)</h2><p>前面定義出了Batch Hard的Triplet Loss是怎麼運作的，接下來要介紹什麼是Semi-Hard的Tirplet，簡而言之就是要找那些AN&gt;AP但&lt;margin的pair。</p>
<p>這裡分成5個部分來分別介紹。</p>
<h3 id="Distance-Metric-1"><a href="#Distance-Metric-1" class="headerlink" title="Distance Metric"></a>Distance Metric</h3><p>和前面一樣先計算出<code>pdist_matrix</code>，和相應的<code>adjacency</code> mask跟<code>adjacency_not</code> mask：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pdist_matrix = _distance_metric(embedding, squared=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">adjacency = np.equal(labels, tf.transpose(labels)).astype(np.float32)</span><br><span class="line">adjacency_not = np.logical_not(adjacency).astype(np.float32)</span><br></pre></td></tr></table></figure>

<h3 id="negatives-outside"><a href="#negatives-outside" class="headerlink" title="negatives_outside"></a>negatives_outside</h3><p>將<code>pdist_matrix</code>複製batch次，會得到<code>pdist_matrix_tile</code>，其shape為(batch*batch, batch)，這是為了之後要挑出Semi-Hard的triplet pair而計算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>同樣將<code>adjacency_not</code>複製batch次，並且將<code>pdist_matrix</code>根據row展開成一列其shape為(batch*batch, 1)，讓<code>pdist_matrix_tile</code>中每一個element依序和展開的<code>pdist_matrix</code>相比，大於為Ture、反之False，最後再和batch倍的adjacency_not(為negative的所在)做and運算，例子如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">np.reshape(np.transpose(pdist_matrix), [-1, 1])</span></span><br><span class="line"><span class="string">------------------------------------</span></span><br><span class="line"><span class="string">ex. pdist_matrix = [[ 0. 11. 11.]</span></span><br><span class="line"><span class="string">                    [11.  0. 24.]</span></span><br><span class="line"><span class="string">                    [11. 24.  0.]]</span></span><br><span class="line"><span class="string">轉換成(batch*batch,1):</span></span><br><span class="line"><span class="string">[[ 0.], [11.], [11.], [11.], [ 0.], [24.], [11.], [24.], [ 0.]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">依序和展開的`pdist_matrix`相比</span></span><br><span class="line"><span class="string">np.greater(pdist_matrix_tile, np.reshape(np.transpose(pdist_matrix), [-1, 1]))</span></span><br><span class="line"><span class="string">------------------------------------</span></span><br><span class="line"><span class="string">[[False  True  True]</span></span><br><span class="line"><span class="string"> [False False  True]</span></span><br><span class="line"><span class="string"> [False  True False]</span></span><br><span class="line"><span class="string"> [False False False]</span></span><br><span class="line"><span class="string"> [ True False  True]</span></span><br><span class="line"><span class="string"> [False False False]</span></span><br><span class="line"><span class="string"> [False False False]</span></span><br><span class="line"><span class="string"> [False False False]</span></span><br><span class="line"><span class="string"> [ True  True False]]</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string"> np.logical_and(a, b)</span></span><br><span class="line"><span class="string">------------------------------------</span></span><br><span class="line"><span class="string">[[False  True False]	[[False  True  True]	[[False  True False]</span></span><br><span class="line"><span class="string"> [ True False  True]	 [False False  True]	 [False False  True]</span></span><br><span class="line"><span class="string"> [False  True False]	 [False  True False]	 [False  True False]</span></span><br><span class="line"><span class="string"> [False  True False]	 [False False False]	 [False False False]</span></span><br><span class="line"><span class="string"> [ True False  True] and [ True False  True]  =  [ True False  True]</span></span><br><span class="line"><span class="string"> [False  True False]	 [False False False]	 [False False False]</span></span><br><span class="line"><span class="string"> [False  True False]	 [False False False]	 [False False False]</span></span><br><span class="line"><span class="string"> [ True False  True]	 [False False False]	 [False False False]</span></span><br><span class="line"><span class="string"> [False  True False]]	 [ True  True False]]	 [False  True False]]</span></span><br><span class="line"><span class="string"> &#x27;&#x27;&#x27;</span></span><br><span class="line">a = np.tile(adjacency_not, [batch_size, <span class="number">1</span>])</span><br><span class="line">b = np.greater(pdist_matrix_tile, np.reshape(np.transpose(pdist_matrix), [-<span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">mask = np.logical_and(a, b)</span><br></pre></td></tr></table></figure>

<p>這邊將<code>pdist_matrix</code>中的batch*batch展開，將每一筆當作AP和對應的row去比較(第一筆為row 1對column 1的距離，讓他對第一個row比較，如下圖)，所以這邊評估在每個row中哪些比他本身更大，更大的作為True。</p>
<img src="https://drive.google.com/uc?export=view&id=1uG1hhE55cS6XudRBHEjOB7a8XYuV1lL-" alt="semi-hard_sample" width="70%" height="70%" style="display:block; margin:auto;">

<p>(1.1對1.all比較，1.2對2.all比較，…<br> 2.1對1.all比較，2.2對2.all比較，…<br> 3.1對1.all比較，3.2對2.all比較，…)</p>
<p>接下來透過tile過的<code>adjacency_not</code>，把屬於不同label並且距離大於AP(<code>pdist_matrix</code>中每個element)的都挑選出來了。</p>
<p>接合前面的<code>np_masked_minimum</code> function我們可以得到AN的Semi-Hard的候選清單，也就是AN&gt;AP的情況下最小的AN：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># negatives_outside: smallest D_an where D_an &gt; D_ap.</span></span><br><span class="line">negatives_outside = np.reshape(</span><br><span class="line">np_masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size]</span><br><span class="line">)</span><br><span class="line">negatives_outside = np.transpose(negatives_outside)</span><br></pre></td></tr></table></figure>



<h3 id="negatives-inside"><a href="#negatives-inside" class="headerlink" title="negatives_inside"></a>negatives_inside</h3><p>以上滿足了AP&lt;AN的狀況下找尋可計算pair，但有可能會存在AN&lt;AP的時候，這個時候要找最遠的AN，和AP比較，也就是easy negatives：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># negatives_inside: largest D_an.</span></span><br><span class="line">negatives_inside = np.tile(np_masked_maximum(pdist_matrix, adjacency_not), [<span class="number">1</span>, batch_size])</span><br></pre></td></tr></table></figure>



<h3 id="semi-hard-negatives"><a href="#semi-hard-negatives" class="headerlink" title="semi_hard_negatives"></a>semi_hard_negatives</h3><p>將前面的<code>mask</code>，也就是AN&gt;AP的情況下是否擁有的AN，做<code>reduce_sum</code>。換句話說就是將AN&gt;AP的狀況下存在AN的element為True (也就是semi-hard的部分)，使用<code>negatives_outside</code>的距離，反之用<code>negatives_inside</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">np.where(condition, x, y):</span></span><br><span class="line"><span class="string">if condition:</span></span><br><span class="line"><span class="string">	return x </span></span><br><span class="line"><span class="string">else:</span></span><br><span class="line"><span class="string">	return y</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">mask_final = np.reshape(np.greater(np.<span class="built_in">sum</span>(mask.astype(np.float32), <span class="number">1</span>, keepdims=<span class="literal">True</span>),<span class="number">0.0</span>), [batch_size, batch_size])</span><br><span class="line">mask_final = np.transpose(mask_final)</span><br><span class="line"></span><br><span class="line">semi_hard_negatives = np.where(mask_final, negatives_outside, negatives_inside)</span><br></pre></td></tr></table></figure>

<p>最後在滿足<code>mask_final</code>的情況下選擇<code>negatives_outside</code>和<code>negatives_inside</code>，作為AN來計算Triplet Loss，得到<code>loss_mat</code>。</p>
<h3 id="Semi-Hard-Triplet"><a href="#Semi-Hard-Triplet" class="headerlink" title="Semi-Hard Triplet"></a>Semi-Hard Triplet</h3><p>因為在對角線部分為anchor對anchor的距離，這邊需要將它過濾掉，因為不是這邊需要的，所以算出label相同的mask之後減掉對角線，就可以找到positive的位置，將前面的<code>loss_mat</code>乘上這邊的mask，算總和後除以positive數量，就可以得到最後的triplet loss：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">loss_mat = np.add(margin, pdist_matrix - semi_hard_negatives)</span><br><span class="line"></span><br><span class="line">mask_positives = adjacency.astype(np.float32) - np.diag(np.ones([batch_size]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Take all positive pairs except the diagonal.</span></span><br><span class="line">num_positives = tf.math.reduce_sum(mask_positives)</span><br><span class="line"></span><br><span class="line">triplet_loss = tf.math.truediv(</span><br><span class="line">  tf.math.reduce_sum(</span><br><span class="line">    tf.math.maximum(tf.math.multiply(loss_mat, mask_positives), <span class="number">0.0</span>)</span><br><span class="line">  ),</span><br><span class="line">  num_positives,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h2 id="Code-Link"><a href="#Code-Link" class="headerlink" title="Code Link"></a>Code Link</h2><p>以上所有的code都是在<code>Google Colab</code>上面執行，版本為<code>2.2.0</code>。</p>
<p>Github：<a target="_blank" rel="noopener" href="https://github.com/AugustusHsu/Blogger-Code/tree/master/NeuralNetwork-01%20Batch%20Hard%20and%20Semi-Hard%20Triplet%20Loss"><strong>NeuralNetwork-01 Batch Hard and Semi-Hard Triplet Loss</strong></a></p>
<h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><p><a target="_blank" rel="noopener" href="https://github.com/tensorflow/addons/blob/v0.9.1/tensorflow_addons/losses/triplet.py">Triplet-Loss原理及其实现</a> </p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Neural Network-01 Batch Hard and Semi-Hard Triplet Loss</p><p><a href="https://augustushsu.github.io/2020/02/04/NeuralNetwork-01/">https://augustushsu.github.io/2020/02/04/NeuralNetwork-01/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Augustus Hsu</p></div></div><div class="level-item is-narrow"><div><h6>發表於</h6><p>2020-02-04</p></div></div><div class="level-item is-narrow"><div><h6>更新於</h6><p>2021-12-03</p></div></div><div class="level-item is-narrow"><div><h6>許可協議</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/triplet-loss/">triplet loss</a><a class="link-muted mr-2" rel="tag" href="/tags/tensorflow/">tensorflow</a><a class="link-muted mr-2" rel="tag" href="/tags/numpy/">numpy</a><a class="link-muted mr-2" rel="tag" href="/tags/batch-hard/">batch hard</a><a class="link-muted mr-2" rel="tag" href="/tags/semi-hard/">semi-hard</a></div><div class="a2a_kit a2a_kit_size_32 a2a_default_style"><a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a><a class="a2a_button_facebook"></a><a class="a2a_button_twitter"></a><a class="a2a_button_telegram"></a><a class="a2a_button_whatsapp"></a><a class="a2a_button_reddit"></a></div><script src="https://static.addtoany.com/menu/page.js" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/06/03/hexo-04/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">hexo系列-04 目前為止遇到的問題</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/12/27/DeepLearning-04/"><span class="level-item">DL Machine系列-04 Docker Hub Automated Build</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">評論</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://augustushsu.github.io/2020/02/04/NeuralNetwork-01/';
            this.page.identifier = '2020/02/04/NeuralNetwork-01/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'augustus-blog' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/MyPhoto.jpg" alt="Augustus"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Augustus</p><p class="is-size-6 is-block">Strike the iron while it is hot</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>New Taipei City, Taiwan</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分類</p><a href="/categories"><p class="title">5</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">標籤</p><a href="/tags"><p class="title">41</p></a></div></div></nav></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">文章目錄</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Triplet-Loss"><span class="level-left"><span class="level-item">1</span><span class="level-item">Triplet Loss</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Sample-Data"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">Sample Data</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Triplet-Loss-Batch-Hard"><span class="level-left"><span class="level-item">2</span><span class="level-item">Triplet Loss (Batch Hard)</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Distance-Metric"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Distance Metric</span></span></a></li><li><a class="level is-mobile" href="#Mask"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">Mask</span></span></a></li><li><a class="level is-mobile" href="#Batch-Hard"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">Batch Hard</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Triplet-Loss-Semi-Hard"><span class="level-left"><span class="level-item">3</span><span class="level-item">Triplet Loss (Semi-Hard)</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Distance-Metric-1"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">Distance Metric</span></span></a></li><li><a class="level is-mobile" href="#negatives-outside"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">negatives_outside</span></span></a></li><li><a class="level is-mobile" href="#negatives-inside"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">negatives_inside</span></span></a></li><li><a class="level is-mobile" href="#semi-hard-negatives"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">semi_hard_negatives</span></span></a></li><li><a class="level is-mobile" href="#Semi-Hard-Triplet"><span class="level-left"><span class="level-item">3.5</span><span class="level-item">Semi-Hard Triplet</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Code-Link"><span class="level-left"><span class="level-item">4</span><span class="level-item">Code Link</span></span></a></li><li><a class="level is-mobile" href="#參考資料"><span class="level-left"><span class="level-item">5</span><span class="level-item">參考資料</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分類</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/DL-Machine/"><span class="level-start"><span class="level-item">DL Machine</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/hexo/"><span class="level-start"><span class="level-item">hexo</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AF%A6%E4%BD%9C/"><span class="level-start"><span class="level-item">實作</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E5%AF%A6%E4%BD%9C/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AF%A6%E4%BD%9C/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><figure class="media-left"><a class="image" href="/2021/12/10/GAN-03/"><img src="/gallery/thumbnails/DCGAN.png" alt="GAN系列-03 Deep Convolutional Generative Adversarial Network"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-12-10T06:00:00.000Z">2021-12-10</time></p><p class="title"><a href="/2021/12/10/GAN-03/">GAN系列-03 Deep Convolutional Generative Adversarial Network</a></p><p class="categories"><a href="/categories/%E5%AF%A6%E4%BD%9C/">實作</a> / <a href="/categories/%E5%AF%A6%E4%BD%9C/GAN/">GAN</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/12/04/GAN-02/"><img src="/gallery/thumbnails/CGAN%20Architecture.jpg" alt="GAN系列-02 Conditional Generative Adversarial Nets"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-12-04T06:00:00.000Z">2021-12-04</time></p><p class="title"><a href="/2021/12/04/GAN-02/">GAN系列-02 Conditional Generative Adversarial Nets</a></p><p class="categories"><a href="/categories/%E5%AF%A6%E4%BD%9C/">實作</a> / <a href="/categories/%E5%AF%A6%E4%BD%9C/GAN/">GAN</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/12/03/GAN-01/"><img src="/gallery/thumbnails/GAN.png" alt="GAN系列-01 Generative Adversarial Network"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-12-03T06:00:00.000Z">2021-12-03</time></p><p class="title"><a href="/2021/12/03/GAN-01/">GAN系列-01 Generative Adversarial Network</a></p><p class="categories"><a href="/categories/%E5%AF%A6%E4%BD%9C/">實作</a> / <a href="/categories/%E5%AF%A6%E4%BD%9C/GAN/">GAN</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/12/01/GAN-00/"><img src="/gallery/thumbnails/GAN.png" alt="GAN系列-00 Generative Adversarial Network 生成對抗網路"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-12-01T06:00:00.000Z">2021-12-01</time></p><p class="title"><a href="/2021/12/01/GAN-00/">GAN系列-00 Generative Adversarial Network 生成對抗網路</a></p><p class="categories"><a href="/categories/%E5%AF%A6%E4%BD%9C/">實作</a> / <a href="/categories/%E5%AF%A6%E4%BD%9C/GAN/">GAN</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2020/06/10/NeuralNetwork-02/"><img src="/uploads/Triplet-Loss.png" alt="Neural Network-02 Triplet Loss Example at MNIST"></a></figure><div class="media-content"><p class="date"><time dateTime="2020-06-10T13:00:32.000Z">2020-06-10</time></p><p class="title"><a href="/2020/06/10/NeuralNetwork-02/">Neural Network-02 Triplet Loss Example at MNIST</a></p><p class="categories"><a href="/categories/%E5%AF%A6%E4%BD%9C/">實作</a> / <a href="/categories/%E5%AF%A6%E4%BD%9C/Neural-Network/">Neural Network</a></p></div></article></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="訂閱"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Augustus</a><p class="is-size-7"><span>&copy; 2021 Augustus Hsu</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/AugustusHsu"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-TW");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到頁首" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此網站使用Cookie來改善您的體驗。",
          dismiss: "知道了！",
          allow: "允許使用Cookie",
          deny: "拒絕",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="請輸入關鍵字..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"請輸入關鍵字...","untitled":"(無標題)","posts":"文章","pages":"頁面","categories":"分類","tags":"標籤"});
        });</script></body></html>